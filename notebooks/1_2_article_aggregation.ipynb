{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d65efe-58c1-40da-8e36-940926480cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26894ee1-0f51-4af8-8560-bf3d134171ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pathlib\n",
    "import newspaper\n",
    "\n",
    "from psycopg2 import connect\n",
    "from urllib3.exceptions import LocationParseError\n",
    "from multiprocessing import Pool\n",
    "from math import floor, ceil\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141e328-23be-4c7b-8884-009840de2f4b",
   "metadata": {},
   "source": [
    "# Article Aggregation \n",
    "\n",
    "There are quite a few source urls in the SQL table. Therefore, it is a good idea for this code to be able to start and stop as needed. The function below will fetch the article text and title given a url, and save them directly to files. No processing is done at this time, just file aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafffd8e-9278-4eb0-8457-747c0249c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_content_fetcher(row):\n",
    "    path = pathlib.Path(\"../data/raw\")\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    _id, url, date, day = row\n",
    "\n",
    "    # create files for the text and title\n",
    "    text_dump = pathlib.Path(path / f\"{_id}.text\")\n",
    "    title_dump = pathlib.Path(path / f\"{_id}.title\")\n",
    "\n",
    "    if not text_dump.is_file():\n",
    "        # if the file already exists, skip it\n",
    "        try:\n",
    "            # there are a lot of things that can prevent successful webscraping\n",
    "            # if nothing throws an error, the text and title are saved\n",
    "            article = newspaper.Article(url=url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            article_text = article.text\n",
    "            article_title = article.title\n",
    "\n",
    "            article_text = article_text if article_text else \"\"\n",
    "            article_title = article_title if article_title else \"\"\n",
    "\n",
    "        # if something does go wrong, empty strings are written instead\n",
    "        except (TypeError, newspaper.ArticleException, LocationParseError):\n",
    "            # TypeError will occur when there is no url\n",
    "            # ArticleException will occur when the response forbids webscraping\n",
    "            # not sure what causes LocationParseError, but it's rare\n",
    "            article_text = \"\"\n",
    "            article_title = \"\"\n",
    "\n",
    "        # the content is written to files\n",
    "        with text_dump.open(mode=\"w+\") as txt, title_dump.open(mode=\"w+\") as ttl:\n",
    "            txt.write(article_text)\n",
    "            ttl.write(article_title)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc048b0-1c23-43ea-b20b-bc06432ffec6",
   "metadata": {},
   "source": [
    "Next, we need a function which will write the relevant content to a table for storage. As the aggregated files are iterated through, this function will write a row for each row in the orginal SQL table it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bd13b0-afd9-49bb-b912-fd07e6725661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_writer(row, conn):\n",
    "    # unpack the row into named variables\n",
    "    _id, url, date, day = row\n",
    "    path = pathlib.Path(\"../data/raw/\")\n",
    "\n",
    "    # get the text and title\n",
    "    text_dump = pathlib.Path(path / f\"{_id}.text\")\n",
    "    title_dump = pathlib.Path(path / f\"{_id}.title\")\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    with text_dump.open(mode=\"r\") as txt, title_dump.open(mode=\"r\") as ttl:\n",
    "        title = title_dump.read_text()\n",
    "        text = text_dump.read_text()\n",
    "        # write the content into the (to be created) source_text table\n",
    "        cmd = f\"INSERT INTO source_text(id, source, date, day, title, text) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "        cursor.execute(\n",
    "            cmd, (_id, url, date, day, title if title else None, text if text else None)\n",
    "        )\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391df489-b688-48a7-97af-9e7fedfca997",
   "metadata": {},
   "source": [
    "Next, have to select a subset of the articles. The following command will:\n",
    "\n",
    "1. Create a new table for the article text\n",
    "2. Group the sources by week to conform with the price data\n",
    "3. Order them randomly\n",
    "4. Select a maximum of 1000 articles per week\n",
    "\n",
    "The limit is implemented to ensure that some weeks don't have an absurd amount of data compared to others, especially because the later years have much more data than earlier years. Additionally, both the formatted date and unformatted date are returned. This is because, due to some error, some dates are returned in the year 1920, which is far earlier than GDELT actually maintains. I wanted to see if the error was on my end or GDELT's (spoiler: it was GDELT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c832b7b9-3a96-485f-b4d2-bf8141021482",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_RANDOM_SEED = 0.42\n",
    "ENTRIES_PER_WEEK = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13041ecc-550e-447a-8c19-5a43ea02ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"\n",
    "      DROP TABLE IF EXISTS source_text;\n",
    "      CREATE TABLE source_text(id integer PRIMARY KEY, source text, date timestamp, day text, title text, text text); -- create table for source text \n",
    "      SELECT SETSEED ({SQL_RANDOM_SEED}); -- set a seed for reproducability\n",
    "      WITH grouped_by_week AS (\n",
    "          SELECT\n",
    "              DISTINCT ON (SOURCEURL) SOURCEURL AS source,\n",
    "              id,\n",
    "              TO_DATE(Day::text, 'YYYYMMDD') AS date,\n",
    "              Day as day,\n",
    "              RANK() OVER (PARTITION BY date_trunc('week', TO_DATE(Day::text, 'YYYYMMDD')) ORDER BY RANDOM()) as row\n",
    "          FROM events\n",
    "      )\n",
    "      SELECT\n",
    "          id,\n",
    "          source,\n",
    "          date,\n",
    "          day\n",
    "      FROM grouped_by_week\n",
    "      WHERE row <= {ENTRIES_PER_WEEK};\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46f10a-0d95-42cc-bc36-933f870ca3f2",
   "metadata": {},
   "source": [
    "Finally, a function to exacute all of the above. Here we implement code which will do the above tasks in a multiprocessed fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84cd507-4fb9-4602-a5e6-d43f1e00788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source article download progress 100% complete\n",
      "112475/112475 rows written to postgreSQL table\n"
     ]
    }
   ],
   "source": [
    "pool = Pool()\n",
    "with open(\"../etc/postgres.password\") as psql_pass_file:\n",
    "    postgres_password = psql_pass_file.read()\n",
    "    conn = connect(\n",
    "        f\"host='localhost' dbname='gdelt' user='postgres' password='{postgres_password}'\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(cmd)\n",
    "    conn.commit()\n",
    "    rows = cursor.fetchall()\n",
    "    print(\"Source article download progress 0% complete\")\n",
    "    # the task is broken down into blocks so that progress can be tracked\n",
    "    for percent in range(100):\n",
    "        # prints progress as percents\n",
    "        bot = floor(percent * len(rows) / 100.0)\n",
    "        top = ceil((percent + 1) * len(rows) / 100.0)\n",
    "\n",
    "        pool.map(article_content_fetcher, rows[bot:top])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Source article download progress {percent + 1}% complete\")\n",
    "    for n, row in enumerate(rows):\n",
    "        row_writer(row, conn)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Source article download progress 100% complete\")\n",
    "        print(f\"{n + 1}/{len(rows)} rows written to postgreSQL table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
