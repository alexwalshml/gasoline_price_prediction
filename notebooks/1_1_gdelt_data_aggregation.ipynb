{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158d4c57-7769-4b95-a83a-5b1076de5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14285e38-514a-46f0-878d-006557d90162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import re\n",
    "import zipfile\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from itertools import chain\n",
    "from psycopg2 import connect\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea47c9-1e66-431f-82cb-56012453e8f0",
   "metadata": {},
   "source": [
    "# Why use NLP for time series forecasting?\n",
    "\n",
    "Time series forecasting is the science of predicting the future, usually for chaotic, less-anticipatable quantities. Weather, stock prices, commodity availability, etc. are all quantities that people wish they could know in advance, the problem being that all of these quantities are often considered to have a component of randomness to them, thus making predicting them a rather futile effort. That's where I disagree. There are few things in the world which are *truly* random, and most of those are limited to fundamental physical phenomena. Everything else is deterministic. Weather is definitively chaotic, but with enough information about the initial state of the system, very accurate predictions can be made. Commodity prices depend on supply and demand, which are simple, but the supply/demand of the components to produce it may be hard to track. Additionally, local problems where the item is produced can have an impact, plus supply-chain issues, changes to foreign import policy, product competitors, and so on on can all impact how much something will cost, but it can be nearly impossible to automate the tracking of such things. However, if one *did* have all of that information, then knowing the change in an item's price in the immediate future should be very achievable. How a stock price will change in the future is usually considered anyone's guess, but I think it would be better to say that it's *everyone's* guess. If more people think an asset will increase in price, that leads to a surplus of buyers and then (usually) the price goes up. The problem is that you have to know what people are thinking *before* they act to make use of that logic. Luckily (or, perhaps, unfortunately), thanks to social media, a large portion of the population can't help but share what they're thinking all the time. By scraping Reddit, Twitter, Facebook, and other social media sites, one could form a solid opinion on how to invest just by using the wisdom of the masses.\n",
    "\n",
    "The point being that these examples (except weather, unless you consider climate change) all have a *human* component. Gas prices are no different. Taxes, emissions policies, what wars are being fought where, who just discovered more oil - all of these things are distinctly human, or at the very least aren't constantly tracked numerically. Federal gasoline tax, for instance, hasn't changed since 1993 [*source*](https://www.pgpf.org/blog/2021/03/its-been-28-years-since-we-last-raised-the-gas-tax-and-its-purchasing-power-has-eroded), so it would be foolish to include its value in a model. But if it were raised, so would the tracked gasoline price. State taxes have changed in that time, but I couldn't find a good source on what the tax rate was for each state going back in time. Not to mention the changes in attitudes on gasoline usage, oil spills, fracking developments, etc. which have occured in the past few years. In all of these cases, *someone* is tracking that information, and they *are* recording it. They just aren't tracking it in some public CSV file that I can load. Instead, they're tracking it in text and reporting it to the world. And *that* is why natural language processing should be implemented, as it is a way to extract as much information about the human component of a problem, and to quantify it.\n",
    "\n",
    "# Where to get the data?\n",
    "\n",
    "The Global Database of Events, Language, and Tone [(GDELT)](https://www.gdeltproject.org/) tracks, well, everything. The project attempts to do two things. First, it tries to create a database of every event that has occured in the world (the past few decades, not all of history), and to try to assign who was involved, when it occured and where, how impactful the event was, how many sources reported on it, and many, many more aspects of the event. Additionally, the store the source URL of the first report on the incident. This is the \"Events\" data. Secondly, are far more impressively, they connect everything together in their \"Gloabl Knowledge Graph\" (GKG). The GKG is massive, and attempts to piece together how different events and people are linked, and it looks for themes present in the event to connect things. However, for the purposes of this project, the GKG data is largely useless because it only expresses connections, and not facts or opinions. The events data doesn't store this either, but it *does* store the source article which, ideally, contains the facts of the situation. Therefore, for each article in the database, I will determine if it is relevant, and, if so, download it.\n",
    "\n",
    "GDELT offers three ways to acquire their data. The first is to download tens of thousands of ZIP files and piece the events database together yourself. This is a ton of data, making this a pretty terrible idea. The second is to use GDELT's own [analysis service](https://www.gdeltproject.org/data.html#gdeltanalysisservice), which allows you to specify some conditions, after which it will fetch the data and email it to you several hours later. Unfortunately, when I attempted to use it, it redirected me to a page stating that the analysis service isn't finished yet, but should be done sometime in 2019. *Good, good*. Luckily, there is a third option, which is to use Google's BigQuery, which is kind enough to store GDELT's tables for public use. Unluckily, some SQL inspection of the tables showed that only the past week of data seemed to be stored, which was entirely useless. I'm not sure if this was a bug, or if Google may have been moving the storage location of the tables, or what. Therefore, we do what any sane person would do at this point and ~curse GDELT~ happily implement option one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302b60fd-e270-4f9f-a7dd-1f0fefb31ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdelt maintains a list of zip files of three types\n",
    "# one type for events, one for gkg, and another for a supplemtary mentions table\n",
    "# step one is to download this lists and parse it\n",
    "# extracting the download locations of the events files\n",
    "def get_master_file_list(url):\n",
    "    r = requests.get(url)\n",
    "    if not r.ok:\n",
    "        # should raise if the list is not reachable\n",
    "        raise BaseException(f\"Invalid HTML status {r.status_code}\")\n",
    "    raw_list = r.text.split(\"\\n\")\n",
    "    event_list = []\n",
    "    for raw in raw_list:\n",
    "        try:\n",
    "            # looks for a url\n",
    "            file_location = re.search(r\"https?://[^\\s]+\", raw).group()\n",
    "        except AttributeError:\n",
    "            # if for some reason the line is poorly formatted/null, we skip the file\n",
    "            pass\n",
    "        \n",
    "        # events files have a .export file type\n",
    "        if \".export\" in file_location:\n",
    "            event_list.append(file_location)\n",
    "\n",
    "    return event_list\n",
    "\n",
    "\n",
    "# next, these csv files are zipped to compress them\n",
    "# because I don't want thousands of zip files on my machine\n",
    "# I will extract and parse them without ever storing the file\n",
    "def fetch_and_parse_zip(url):\n",
    "    r = requests.get(url)\n",
    "    if not r.ok:\n",
    "        # if the file can't be reached, an empty dummy csv is used\n",
    "        csv = open(\"/tmp/gdelt_csv_empty_file.csv\", \"w+\")\n",
    "        return csv\n",
    "    file = r.content\n",
    "    # the zip content is loaded in \n",
    "    zip_object = zipfile.ZipFile(io.BytesIO(file), \"r\")\n",
    "    # and the first item in the list (of one) files is opened\n",
    "    csv = zip_object.open(zip_object.namelist()[0])\n",
    "\n",
    "    # this function returns an open file\n",
    "    return csv\n",
    "\n",
    "# next, we have to get the lines which are actually relevant\n",
    "def get_relevant_events_lines(content, words, aggregate_file):\n",
    "    for line in content.readlines():\n",
    "        try:\n",
    "            # the lines are in binary, so we decode them to a string\n",
    "            line = line.decode(\"utf-8\")\n",
    "            # the source url is extracted from the line\n",
    "            source_url = line.split(\"\\t\")[-1].strip(\"\\n\")\n",
    "            # the url is split on common separators\n",
    "            terms_in_url = re.split(r\"[-._/]+\", source_url)\n",
    "            # the terms are then checked against a list of \"interesting\" words\n",
    "            # if any are present, the line is written to an aggregator file\n",
    "            if any([word in terms_in_url for word in words]):\n",
    "                aggregate_file.write(line)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# this function puts everything together\n",
    "def get_gdelt_events_data(words, progress_file, file_list):\n",
    "    # first, so this code can start and stop, we will maintain a list \\\n",
    "    # in storage of all files which have already been processed\n",
    "    processed_files = [pf.strip() for pf in progress_file.readlines()]\n",
    "    N = len(file_list)\n",
    "    p = pathlib.Path(f\"../data\")\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    q = p / \"gdelt_events.csv\"\n",
    "    with q.open(\"a+\") as agg:\n",
    "        # keep track of time for a nice printout\n",
    "        start_time = time.time()\n",
    "        m = 0\n",
    "        # iterate through the file list\n",
    "        for n, file_url in enumerate(file_list):\n",
    "            if file_url in processed_files:\n",
    "                # if the file has been processed, skip\n",
    "                m += 1\n",
    "                continue\n",
    "            \n",
    "            # get the zip and parse the lines\n",
    "            csv = fetch_and_parse_zip(file_url)\n",
    "            get_relevant_events_lines(csv, words, agg)\n",
    "            \n",
    "            # everything below is just a printout formatter\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            elapsed_time_tuple = str(datetime.timedelta(seconds=elapsed_time)).split(\n",
    "                \":\"\n",
    "            )\n",
    "\n",
    "            elapsed_time_string = f\"{elapsed_time_tuple[0]}:{elapsed_time_tuple[1]}:{round(float(elapsed_time_tuple[2])):02}\"\n",
    "\n",
    "            estimated_time_remaining = (\n",
    "                elapsed_time * (N - m) / (n - m + 1)\n",
    "            ) - elapsed_time\n",
    "\n",
    "            estimated_remaining_time_tuple = str(\n",
    "                datetime.timedelta(seconds=estimated_time_remaining)\n",
    "            ).split(\":\")\n",
    "\n",
    "            estimated_remaining_time_string = f\"{estimated_remaining_time_tuple[0]}:{estimated_remaining_time_tuple[1]}:{round(float(estimated_remaining_time_tuple[2])):02}\"\n",
    "\n",
    "            print(f\"{n + 1}/{N} files parsed\")\n",
    "            print(f\"Elapsed time: {elapsed_time_string}\")\n",
    "            print(f\"Estimated remaining time: {estimated_remaining_time_string}\")\n",
    "\n",
    "            progress_file.write(file_url + \"\\n\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f4a3c-ba7e-4601-8788-eb05c3e6790c",
   "metadata": {},
   "source": [
    "Those functions will do the heavy lifting for processing the data. Now all we have to do is call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df6bb95-fb45-430d-8bd3-ebea5a8ce525",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list = get_master_file_list(\n",
    "    \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2521a8-46f1-47e6-bb9f-b4e480f80ce1",
   "metadata": {},
   "source": [
    "And to define a list of gasoline-related words. It's a very exhaustive list, I know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e3732e-b9b5-4947-9964-fd924b9277de",
   "metadata": {},
   "outputs": [],
   "source": [
    "OIL_WORDS = [\n",
    "    \"oil\", # NOTE: as oil does not strictly refer to petroleum, we will also get some articles on olive oil, etc\n",
    "    \"gas\", # same with gas and the state of matter\n",
    "    \"gasoline\",\n",
    "    \"petrol\",\n",
    "    \"fuel\",\n",
    "    \"petroleum\",\n",
    "    \"diesel\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864581f-628d-4db5-935e-b292025dfeaa",
   "metadata": {},
   "source": [
    "Now we run the main function. If the function has never been run, the progress file must be created. Otherwise, it is read in from memory. To restart data aggregation, all one has to do is delete the existing `progress.txt` file.\n",
    "\n",
    "(NOTE: The elapsed time output of this cell is indicative of how long it took the cell to run that time, and not a cumulative counter. It currently reads ten minutes, but if it were run from the beginning this cell would take roughly twenty hours to run on my network.)\n",
    "\n",
    "(SECOND NOTE: I opted to not use multiprocessing to speed up this code. This is because I believe the functions to be IO limited and not CPU limited, so multiprocessing is likely to get very messy and offer little improvement.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea4c5fa-1b2c-413b-9045-7dfdf83c9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272839/272839 files parsed\n",
      "Elapsed time: 0:10:20\n",
      "Estimated remaining time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"../data/events_progress.txt\", \"r+\") as progress_file:\n",
    "        get_gdelt_events_data(OIL_WORDS, progress_file, events_list)\n",
    "except FileNotFoundError:\n",
    "    with open(\"../data/events_progress.txt\", \"w+\") as progress_file:\n",
    "        get_gdelt_events_data(OIL_WORDS, progress_file, events_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15ff9f-a7ee-4b3a-8eae-0cc39a54e401",
   "metadata": {},
   "source": [
    "# SQL Hosting\n",
    "\n",
    "Once all the data is gathered, we will create a database to hold all the information. The cell below will execute terminal commands to create a fresh database. To execute it, one must alter the files in `etc/` to contain your sudoers password in `user.password`, as well as your postgres password in `postgres.password`. (Good thing this isn't a cybersecurity project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7137ec7-dcb5-49b6-a236-cb10244baa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! sudo -S -i -u postgres dropdb -f gdelt < ../etc/user.password\n",
    "! sudo -S -i -u postgres createdb gdelt < ../etc/user.password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3ed996-f72e-47ce-9f5a-397cffb0bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a string of column names and types to save room in the sql command\n",
    "# id is not a data column found in the gdelt data, but we add it to avoid trouble with duplicates\n",
    "\n",
    "event_columns = \"\"\"id serial PRIMARY KEY,\n",
    "                   GlobalEventID integer, \n",
    "                   Day integer,\n",
    "                   MonthYear integer,\n",
    "                   Year integer,\n",
    "                   FractionDate numeric,\n",
    "                   Actor1Code text,\n",
    "                   Actor1Name text,\n",
    "                   Actor1CountryCode text,\n",
    "                   Actor1KnownGroupCode text,\n",
    "                   Actor1EthnicCode text,\n",
    "                   Actor1Religion1Code text,\n",
    "                   Actor1Religion2Code text,\n",
    "                   Actor1Type1Code text,\n",
    "                   Actor1Type2Code text,\n",
    "                   Actor1Type3Code text,\n",
    "                   Actor2Code text,\n",
    "                   Actor2Name text,\n",
    "                   Actor2CountryCode text,\n",
    "                   Actor2KnownGroupCode text,\n",
    "                   Actor2EthnicCode text,\n",
    "                   Actor2Religion1Code text,\n",
    "                   Actor2Religion2Code text,\n",
    "                   Actor2Type1Code text,\n",
    "                   Actor2Type2Code text,\n",
    "                   Actor2Type3Code text,\n",
    "                   IsRootEvent integer,\n",
    "                   EventCode text,\n",
    "                   EventBaseCode text,\n",
    "                   EventRootCode text,\n",
    "                   QuadClass integer,\n",
    "                   GoldsteinScale text,\n",
    "                   NumMentions integer,\n",
    "                   NumSources integer,\n",
    "                   NumArticles integer,\n",
    "                   AvgTone numeric,\n",
    "                   Actor1Geo_Type integer,\n",
    "                   Actor1Geo_Fullname text,\n",
    "                   Actor1Geo_CountryCode text,\n",
    "                   Actor1Geo_ADM1Code text,\n",
    "                   Actor1Geo_ADM2Code text,\n",
    "                   Actor1Geo_Lat text,\n",
    "                   Actor1Geo_Long text,\n",
    "                   Actor1Geo_FeatureID text,\n",
    "                   Actor2Geo_Type integer,\n",
    "                   Actor2Geo_Fullname text,\n",
    "                   Actor2Geo_CountryCode text,\n",
    "                   Actor2Geo_ADM1Code text,\n",
    "                   Actor2Geo_ADM2Code text,\n",
    "                   Actor2Geo_Lat text,\n",
    "                   Actor2Geo_Long text,\n",
    "                   Actor2Geo_FeatureID text,\n",
    "                   ActionGeo_Type integer,\n",
    "                   ActionGeo_Fullname text,\n",
    "                   ActionGeo_CountryCode text,\n",
    "                   ActionGeo_ADM1Code text,\n",
    "                   ActionGeo_ADM2Code text,\n",
    "                   ActionGeo_Lat text,\n",
    "                   ActionGeo_Long text,\n",
    "                   ActionGeo_FeatureID text,\n",
    "                   DATEADDED bigint,\n",
    "                   SOURCEURL text\"\"\"\n",
    "\n",
    "# NOTE: strictly speaking, many columns (such as GoldsteinScale)\n",
    "# should be of type numeric. However, these columns contain\n",
    "# empty strings in the .csv files. This will cause issues\n",
    "# in the COPY FROM postgreSQL command. Therefore, we treat\n",
    "# these columns as text initially. If reason is found to use\n",
    "# these columns in the analysis, the columns can be converted later\n",
    "# within the database or via python after the data is fetched\n",
    "\n",
    "# we also want a string of column names to actually insert the data\n",
    "event_columns_no_types_no_id = (\n",
    "    event_columns.replace(\" text\", \"\")\n",
    "    .replace(\" numeric\", \"\")\n",
    "    .replace(\" bigint\", \"\")\n",
    "    .replace(\" integer\", \"\")\n",
    "    .replace(\"id serial PRIMARY KEY,\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ebc310-7fd9-4c6f-9a70-609ba01805dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres\n"
     ]
    }
   ],
   "source": [
    "# read in the postgres password to form the connection\n",
    "with open(\"../data/gdelt_events.csv\", \"r\") as events_file, open(\n",
    "    \"../etc/postgres.password\"\n",
    ") as psql_pass_file:\n",
    "    postgres_password = psql_pass_file.read()\n",
    "    # form the connection\n",
    "    conn = connect(\n",
    "        f\"host='localhost' dbname='gdelt' user='postgres' password='{postgres_password}'\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    # the database is fresh, so create an events table\n",
    "    create_events_table_cmd = f\"CREATE TABLE events({event_columns})\"\n",
    "    # copy from the aggregated csv\n",
    "    copy_events_cmd = f\"COPY events({event_columns_no_types_no_id}) FROM STDIN WITH (FORMAT TEXT, HEADER FALSE)\"\n",
    "    cursor.execute(create_events_table_cmd)\n",
    "    cursor.copy_expert(copy_events_cmd, events_file)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c90843-1b8b-4d5e-8599-e29d16bb0ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
